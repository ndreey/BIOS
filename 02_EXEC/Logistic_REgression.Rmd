---
title: "Logistic Regression"
author: "Andre Bourbonnais"
date: "2023-11-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Logistic Regression
```{r}
library(tidyverse)
library(MASS)
library(flexplot)

```

## Poisson and negative-bionomial regression












# Logistic Regression - StatQuest Example

Following [this video](https://www.youtube.com/watch?v=C4N3_XJJ-jU&list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe&index=7) from StatQuest.

## Getting the data and cleaning it up.



```{r}
url <- "https://raw.githubusercontent.com/StatQuest/logistic_regression_demo/master/processed.cleveland.data"
 
data <- read.csv(url, header=FALSE)
 
#####################################
##
## Reformat the data so that it is
## 1) Easy to use (add nice column names)
## 2) Interpreted correctly by glm()..
##
#####################################
head(data) # you see data, but no column names
 
colnames(data) <- c(
  "age",
  "sex",# 0 = female, 1 = male
  "cp", # chest pain
  # 1 = typical angina,
  # 2 = atypical angina,
  # 3 = non-anginal pain,
  # 4 = asymptomatic
  "trestbps", # resting blood pressure (in mm Hg)
  "chol", # serum cholestoral in mg/dl
  "fbs",  # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE
  "restecg", # resting electrocardiographic results
  # 1 = normal
  # 2 = having ST-T wave abnormality
  # 3 = showing probable or definite left ventricular hypertrophy
  "thalach", # maximum heart rate achieved
  "exang",   # exercise induced angina, 1 = yes, 0 = no
  "oldpeak", # ST depression induced by exercise relative to rest
  "slope", # the slope of the peak exercise ST segment
  # 1 = upsloping
  # 2 = flat
  # 3 = downsloping
  "ca", # number of major vessels (0-3) colored by fluoroscopy
  "thal", # this is short of thalium heart scan
  # 3 = normal (no cold spots)
  # 6 = fixed defect (cold spots during rest and exercise)
  # 7 = reversible defect (when cold spots only appear during exercise)
  "hd" # (the predicted attribute) - diagnosis of heart disease
  # 0 if less than or equal to 50% diameter narrowing
  # 1 if greater than 50% diameter narrowing
)
 
head(data) # now we have data and column names
 
str(data) # this shows that we need to tell R which columns contain factors
# it also shows us that there are some missing values. There are "?"s
# in the dataset. These are in the "ca" and "thal" columns...
 
## First, convert "?"s to NAs...
data[data == "?"] <- NA

# Now add factors for variables that are factors and clean up the factors
## that had missing data...
data[data$sex == 0,]$sex <- "F"
data[data$sex == 1,]$sex <- "M"
data$sex <- as.factor(data$sex)
 
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$restecg <- as.factor(data$restecg)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)
 
data$ca <- as.integer(data$ca) # since this column had "?"s in it
# R thinks that the levels for the factor are strings, but
# we know they are integers, so first convert the strings to integers...
data$ca <- as.factor(data$ca)  # ...then convert the integers to factor levels
 
data$thal <- as.integer(data$thal) # "thal" also had "?"s in it.
data$thal <- as.factor(data$thal)
 
## This next line replaces 0 and 1 with "Healthy" and "Unhealthy"
data$hd <- ifelse(test=data$hd == 0, yes="Healthy", no="Unhealthy")
data$hd <- as.factor(data$hd) # Now convert to a factor
 
str(data) ## this shows that the correct columns are factors
 
## Now determine how many rows have "NA" (aka "Missing data"). If it's just
## a few, we can remove them from the dataset, otherwise we should consider
## imputing the values with a Random Forest or some other imputation method.
nrow(data[is.na(data$ca) | is.na(data$thal),])
data[is.na(data$ca) | is.na(data$thal),]
## so 6 of the 303 rows of data have missing values. This isn't a large
## percentage (2%), so we can just remove them from the dataset
## NOTE: This is different from when we did machine learning with
## Random Forests. When we did that, we imputed values.
nrow(data)
data <- data[!(is.na(data$ca) | is.na(data$thal)),]
nrow(data)

```

## Exploratory Data Analysis

Now we need to make sure that healthy and diseased samples come from each gender. If only male samples have heart disease, we should probably remove all females from the model. This can be controlled using the `xtabs()` function. By selecting the data frame and the columns, we can build a new table.

```{r}
# Inspecting.. 
xtabs(~ hd + sex, data=data)


# xtabs(~ hd + cp, data=data)
# xtabs(~ hd + fbs, data=data)
# xtabs(~ hd + exang, data=data)
# xtabs(~ hd + slope, data=data)
# xtabs(~ hd + ca, data=data)
# xtabs(~ hd + thal, data=data)

```

We see that **healthy** and **unhealthy** patients are both represented by a lot of female and male samples. This is good. We can do the same thing for all the **boolean** and **categorical** variables that we are using to predict heart disease.

Here we see something that can be trouble. Only 4 patients represent level 1. This, could potentially, get in the way of finding the best fitting line. However, for now we will just leave it in and see what happens.

```{r}
xtabs(~ hd + restecg, data=data)

```

## Logistic Regression

Lets start with a super simple model. We will try to predict heart disease using only the gender of each patient.

```{r}
logistic <- glm(hd ~ sex, data=data, family="binomial")
summary(logistic)


```

The coefficients describe this following model: $$\large \text{heart disease} = -1.0438 + 1.2737 * \text{the patient is male}$$

Thus, the log(odds) that a female has heart disease (male = 0) is $\large -1.0438$. If we were predicting for a male patient (x = 1) we get this: $$\large \text{heart disease} = -1.0438 + 1.2737 * 1$$ Since the first term is the log(odds) of a female having heart disease, the second term indicates the increase in the log(odds) that a male has of having heart disease. In other words, the second term is the log(odds ratio) of the odds that a male will have heart disease over the odds that a female will have heart disease.

The p-values are significant but remember that a small p-value alone is not interesting, we also want large **effect sizes**, and thats what the log(odds) and log(odds ratio) tells us.

The **null deviance** and **residual deviance** can be used to compare models, compute $\large r^2$ and overall p-value.

Then we have the **AIC**. These are used to compare models. The model with the lowest AIC is the best model. In this context, the AIC is just the *residual deviance* adjusted for the number of parameters in the model.

Lastly, number of Fisher Scoring tells us how quickly the glm function converged on the maximum likelihood estimates for the coefficients.

## Fancy model

Now that we have done a simple model, lets try a more complicated model. We will use all the variables that we have.

```{r}

fancy_lr <- glm(hd ~ ., data=data, family="binomial")
summary(fancy_lr)

```

This is giving us a large summary, luckily we can focus on only a few coefficients.

-   **AGE:** We see that age isnt a useful predictor because it has a large p-value. The median age was 56, so most of the folks were pretty old and that explains why it wasnt very useful.

-   **GENDER:** Gender is still a good predictor.

If we look at the Residual Deviance and the AIC, we see that both are much smaller for this fancy model than they were for the simple model. This means that the fancy model is a better fit for the data.

If we want to calculate the McFaddens Pseudo $\large r^2$, we can pull the log-likelihood of the null model out of the model by getting the value for the null deviance and dividing by -2. Then we can pull the log-likelihood for the fancy model out of the model by getting the value for the residual deviance and dividing by -2. Then we can just do the math and we end up with a Pseudo $\large r^2$ of 0.55. This can be interpreted as the overall effect size.

```{r}
ll.null <- fancy_lr$null.deviance / -2
ll.proposed <- fancy_lr$deviance / -2
(ll.null - ll.proposed) / ll.null


```

P-value

```{r}
1-pchisq(2*(ll.proposed - ll.null), df=(length(fancy_lr$coefficients)-1))

```

## Plotting

```{r}
## now we can plot the data
predicted.data <- data.frame(
  probability.of.hd=fancy_lr$fitted.values,
  hd=data$hd)
 
predicted.data <- predicted.data[
  order(predicted.data$probability.of.hd, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
 
## Lastly, we can plot the predicted probabilities for each sample having
## heart disease and color by whether or not they actually had heart disease
ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
  geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of getting heart disease")

```
